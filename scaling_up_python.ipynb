{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas and Beyond, Scaling Python Up To Bigger Data and Faster Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Pandas Where You Can, Scale Up Where You Must"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"time_space.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas is talked about more on Stack Overflow than a lot of popular programming languages (currently talked about as much as C++)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"pandas_on_so.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to run can become excessive because either the CPU / GPU is crunching many numbers or because we're blocked waiting on some data or resource to become available\n",
    "\n",
    "CPUs run much faster than human brains - depending on where you read data from the CPU has to wait vastly different amounts of time even though the time difference might be relatively small at the human scale.\n",
    "\n",
    "### Analogy for comparing human and CPU time for IO bound tasks\n",
    "\n",
    "| Human Time | CPU Time |\n",
    "| - | - |\n",
    "| Fetching papers on your desktop and reading them | Fetching data from the CPU cache memory |\n",
    "| Searching your desk draw for a paper - put it on the desktop and reading it | Fetching data from RAM memory |\n",
    "| Travel to your local library, pick up a book, return to your desk and read it | Fetching data from your SSD, HDD, or external disk drive |\n",
    "| Travel to your local library, create inter-library loan request, leave, come back when the book arrives, return with it to your desk | Reading data from a database or API from another computer on the network or Internet |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Pandas Example - Download Australian Energy Market Operator (AEMO) Supervisory Control and Data Acquisition (SCADA) Data and join into a single table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = pd.date_range(start=\"Sept 2006\", end=pd.Timestamp.today(), freq=\"m\")\n",
    "csv_file_list = [\n",
    "    f\"\"\"http://data.wa.aemo.com.au/datafiles/facility-scada/facility-scada-{date.strftime(\"%Y-%m\")}.csv\"\"\"\n",
    "    for date in date_range\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:14:46.011097Z",
     "iopub.status.busy": "2020-10-29T14:14:46.010672Z",
     "iopub.status.idle": "2020-10-29T14:35:30.538869Z",
     "shell.execute_reply": "2020-10-29T14:35:30.538255Z",
     "shell.execute_reply.started": "2020-10-29T14:14:46.011048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bef211121ef4611ae78fb693cfd8c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=169.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owen/miniconda3/envs/jupyter/lib/python3.8/site-packages/IPython/core/magic.py:187: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  call = lambda f, *a, **k: f(*a, **k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 22.4 s, sys: 13.7 s, total: 36.1 s\n",
      "Wall time: 20min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "facility_scada_df = pd.concat(\n",
    "    [pd.read_csv(csv_file) for csv_file in tqdm(csv_file_list)], ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T14:35:30.569266Z",
     "iopub.status.busy": "2020-10-29T14:35:30.569043Z",
     "iopub.status.idle": "2020-10-29T14:35:34.523501Z",
     "shell.execute_reply": "2020-10-29T14:35:34.521898Z",
     "shell.execute_reply.started": "2020-10-29T14:35:30.569216Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "facility_scada_df.to_parquet(\"facility_scada.snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facility_scada_df = pd.read_parquet(\"facility_scada.snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facility_scada_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facility_scada_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show off how easily we can maniuplate and plot data with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "facility_scada_df = pd.read_parquet(\"facility_scada.snappy\")\n",
    "facility_scada_df[\"Participant Facility\"] = facility_scada_df[\n",
    "    \"Participant Code\"\n",
    "].str.cat(facility_scada_df[\"Facility Code\"], sep=\" - \")\n",
    "\n",
    "aemo_scada_pivot_df = facility_scada_df.pivot(\n",
    "    index=\"Trading Interval\",\n",
    "    columns=\"Participant Facility\",\n",
    "    values=\"Energy Generated (MWh)\",\n",
    ")\n",
    "aemo_scada_pivot_df.index = pd.to_datetime(aemo_scada_pivot_df.index)\n",
    "aemo_scada_pivot_df /= 1000.0\n",
    "top_10_generators = aemo_scada_pivot_df.sum().sort_values(ascending=False).index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aemo_scada_pivot_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aemo_scada_pivot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with plt.style.context('seaborn-bright'):\n",
    "    ax = aemo_scada_pivot_df.loc[:, top_10_generators].resample(\"D\").sum().cumsum().plot(\n",
    "        figsize=(12, 6),\n",
    "        title=\"Energy Generated\"\n",
    "    )\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Energy Generated (GWh)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try downloading those CSV files again - but use all the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-03T15:38:40.174978Z",
     "iopub.status.busy": "2020-11-03T15:38:40.174835Z",
     "iopub.status.idle": "2020-11-03T15:38:40.278494Z",
     "shell.execute_reply": "2020-11-03T15:38:40.277993Z",
     "shell.execute_reply.started": "2020-11-03T15:38:40.174964Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-03T15:38:40.783989Z",
     "iopub.status.busy": "2020-11-03T15:38:40.783690Z",
     "iopub.status.idle": "2020-11-03T15:38:40.789434Z",
     "shell.execute_reply": "2020-11-03T15:38:40.788604Z",
     "shell.execute_reply.started": "2020-11-03T15:38:40.783957Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T13:16:03.316792Z",
     "iopub.status.busy": "2020-10-29T13:16:03.316172Z",
     "iopub.status.idle": "2020-10-29T13:20:41.927680Z",
     "shell.execute_reply": "2020-10-29T13:20:41.927045Z",
     "shell.execute_reply.started": "2020-10-29T13:16:03.316720Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18eed07a9ce9462ea1bde045bca67723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=169.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 2.44 s, sys: 1.15 s, total: 3.59 s\n",
      "Wall time: 4min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "facility_scada_parallel_df = pd.concat(\n",
    "    Parallel(n_jobs=-1)(\n",
    "        delayed(pd.read_csv)(csv_file) for csv_file in tqdm(csv_file_list)\n",
    "    ),\n",
    "    ignore_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:20:41.755941Z",
     "iopub.status.busy": "2020-10-29T15:20:41.755800Z",
     "iopub.status.idle": "2020-10-29T15:20:44.966366Z",
     "shell.execute_reply": "2020-10-29T15:20:44.965911Z",
     "shell.execute_reply.started": "2020-10-29T15:20:41.755926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facility_scada_parallel_df.equals(facility_scada_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try downloading those CSV files again - but Asynchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T13:56:46.205075Z",
     "iopub.status.busy": "2020-10-29T13:56:46.204902Z",
     "iopub.status.idle": "2020-10-29T13:56:46.253430Z",
     "shell.execute_reply": "2020-10-29T13:56:46.252789Z",
     "shell.execute_reply.started": "2020-10-29T13:56:46.205061Z"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T13:56:47.189639Z",
     "iopub.status.busy": "2020-10-29T13:56:47.189500Z",
     "iopub.status.idle": "2020-10-29T13:56:47.192794Z",
     "shell.execute_reply": "2020-10-29T13:56:47.192272Z",
     "shell.execute_reply.started": "2020-10-29T13:56:47.189625Z"
    }
   },
   "outputs": [],
   "source": [
    "async def download_file(session: aiohttp.ClientSession, file_url: str):\n",
    "    async with session.get(file_url) as resp:\n",
    "        return pd.read_csv(io.BytesIO(await resp.read()))\n",
    "\n",
    "\n",
    "async def download_file_list(file_url_list):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [download_file(session, file_url) for file_url in file_url_list]\n",
    "        return await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T13:56:51.126774Z",
     "iopub.status.busy": "2020-10-29T13:56:51.126602Z",
     "iopub.status.idle": "2020-10-29T13:57:37.298230Z",
     "shell.execute_reply": "2020-10-29T13:57:37.297485Z",
     "shell.execute_reply.started": "2020-10-29T13:56:51.126759Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owen/miniconda3/envs/jupyter/lib/python3.8/asyncio/base_events.py:570: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  self._run_once()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time:      46.17s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "facility_scada_async_df = pd.concat(await download_file_list(csv_file_list), ignore_index=True)\n",
    "t1 = time.time()\n",
    "print(f\"Wall time: {t1 - t0:10.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T15:20:48.772052Z",
     "iopub.status.busy": "2020-10-29T15:20:48.771879Z",
     "iopub.status.idle": "2020-10-29T15:20:51.912747Z",
     "shell.execute_reply": "2020-10-29T15:20:51.912278Z",
     "shell.execute_reply.started": "2020-10-29T15:20:48.772038Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facility_scada_async_df.equals(facility_scada_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have a look at the New York Taxi Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the individual NY Taxi CSV files [here](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ny_taxi_june_2019_df = pd.read_csv(\"yellow_tripdata_2019-06.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_usage_bytes = ny_taxi_june_2019_df.memory_usage().sum()\n",
    "print(f\"DataFrame memory usage (Mb): {memory_usage_bytes / 1024 / 1024}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ny_taxi_june_2019_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ny_taxi_june_2019_df[\"trip_distance\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "ny_taxi_june_2019_df[\"trip_distance\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-01T08:54:59.329288Z",
     "iopub.status.busy": "2020-11-01T08:54:59.328674Z",
     "iopub.status.idle": "2020-11-01T08:54:59.335993Z",
     "shell.execute_reply": "2020-11-01T08:54:59.334035Z",
     "shell.execute_reply.started": "2020-11-01T08:54:59.329214Z"
    }
   },
   "source": [
    "## Scaling up with CuDF (Linux only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ny_taxi_june_2019_cdf = cudf.read_csv(\"yellow_tripdata_2019-06.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_usage_bytes = ny_taxi_june_2019_cdf.memory_usage().sum()\n",
    "print(f\"DataFrame memory usage (Mb): {memory_usage_bytes / 1024 / 1024}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ny_taxi_june_2019_cdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ny_taxi_june_2019_cdf[\"trip_distance\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "ny_taxi_june_2019_cdf[\"trip_distance\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling up with Vaex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vaex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the follow dataset file [here](https://vaex.readthedocs.io/en/latest/datasets.html) but do it at work unless you have unlimited downloads because it is over 100 gigs. There's a smaller 1 year dataset file which is about 12 gigs there too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ny_tax_2009_2015_df = vaex.open('yellow_taxi_2009_2015_f32.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_tax_2009_2015_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ny_tax_2009_2015_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ny_tax_2009_2015_df[\"trip_distance\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "long_min = -74.05\n",
    "long_max = -73.75\n",
    "lat_min = 40.58\n",
    "lat_max = 40.90\n",
    "\n",
    "ax = ny_tax_2009_2015_df.plot(\n",
    "    ny_tax_2009_2015_df[\"pickup_longitude\"],\n",
    "    ny_tax_2009_2015_df[\"pickup_latitude\"],\n",
    "    f=\"log1p\",\n",
    "    limits=[[long_min, long_max], [lat_min, lat_max]],\n",
    "    show=True,\n",
    "    shape=2048,\n",
    "    figsize=(18, 12),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale up even more with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dask Client, Insert Code, Show Graph Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ny_taxi_df = dd.read_parquet(\"yellow_taxi_2009_2015_f32.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_taxi_df.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ny_taxi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ny_taxi_df[\"trip_distance\"].mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beware - Native Python Types are Flexible but Very Memory Expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because everything is an object in Python even simple types like integers carry a lot of admin baggage - one number uses 28 bytes of memory - this could be 1, 2, or 4 bytes in other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Python list of a million integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_python_list = [number for number in range(1,1_000_000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_million_integers_in_list = sys.getsizeof(a_python_list) + len(a_python_list) * sys.getsizeof(a_python_list[0])\n",
    "print(f\"A Python list of a million integers in a list takes {size_of_million_integers_in_list / 1024 / 1024:5.2f} Mb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Numpy array with the same integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_numpy_array = np.arange(1,1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_numpy_array = sys.getsizeof(a_numpy_array)\n",
    "print(f\"A numpy array of a million integers in a list takes {size_of_numpy_array / 1024 / 1024:5.2f} Mb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T15:33:24.436074Z",
     "iopub.status.busy": "2020-10-07T15:33:24.436074Z",
     "iopub.status.idle": "2020-10-07T15:33:24.440075Z",
     "shell.execute_reply": "2020-10-07T15:33:24.439072Z",
     "shell.execute_reply.started": "2020-10-07T15:33:24.436074Z"
    }
   },
   "source": [
    "## Python is Slow To Run - How Can We Make it Fast? Use Numba!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example use case - Approximating PI using the Monte Carlo Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from numba import jit\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"Pi_30K.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_pi_slow(nsamples: int):\n",
    "    \"\"\"\n",
    "    Approximate PI by taking lots of random samples of points inside a square\n",
    "    overlapped by a quarter circle and counting the ratio of points that fall\n",
    "    inside the quarter circle relative to the total number of sample points.\n",
    "    \"\"\"\n",
    "    acc = 0\n",
    "    for i in range(nsamples):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        if (x**2 + y**2) < 1.0:\n",
    "            acc += 1\n",
    "    return 4.0 * acc / nsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def monte_carlo_pi_fast(nsamples: int):\n",
    "    \"\"\"\n",
    "    Approximate PI by taking lots of random samples of points inside a square\n",
    "    overlapped by a quarter circle and counting the ratio of points that fall\n",
    "    inside the quarter circle relative to the total number of sample points.\n",
    "    \"\"\"\n",
    "    acc = 0\n",
    "    for i in range(nsamples):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        if (x**2 + y**2) < 1.0:\n",
    "            acc += 1\n",
    "    return 4.0 * acc / nsamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plain Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "monte_carlo_pi_slow(10_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First call = compile + runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "monte_carlo_pi_fast(10_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsequent calls = just runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "monte_carlo_pi_fast(10_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### 1. Use Pandas where you can\n",
    "\n",
    "Pandas is single-threaded but it is fast. Take advantage of its vectorizing features when doing calculations or adding calculated columns. You should almost never need to loop over rows of a Pandas Dataframe or Series.\n",
    "\n",
    "### 2. If you start running out of space try the following\n",
    "\n",
    "1. If your data still fits comfortably on your local SSD or HDD then take advantage of lazy loading packages like MMap, Vaex, Dask, or try a simple database like SQL Lite.\n",
    "2. If your data is larger than can fit on a local drive you'll need to start using a full database management system.\n",
    "\n",
    "### 3. If your code runs too slow \n",
    "\n",
    "1. Identify whether the code is slow because it is IO bound and is spending a lot of time waiting for data to move around, or whether it is slow because it is CPU bound from crunching so many numbers.\n",
    "2. If your code is IO bound try the asyncio library and look for async compatible packages, e.g. aiohttp / httpx for asynchronous web requests, aiofiles for asynchronous file reads and writes, and there's many other packages for aynchronous database requests and more.\n",
    "3. Look for opportunities to parallelize the computation (i.e. across rows of a table, across of loop of calculations, or any other architectures that repeat a lot of calculations independently of each other). If using scikit-learn functions look for the \"n_jobs\" parameter and set that to -1 to use all your CPU cores (scikit-learn integrates with the joblib package).\n",
    "4. Try joblib to parallelize work across all your CPU cores, if you have NVidia GPU compute use packages like cudf and cupy (In some cases a small computer with GPU might be faster and cheaper than a cluster of CPU only computers). If you have access to a cluster of computers like a Dask or Spark cluster you can use joblib to parallelize work across a whole cluster of computers and operations with. For the ultimate in parallelizing work look at the [rapids.ai project](https://rapids.ai/) which uses Dask to parallelize across a cluster of computers with GPUs for massively parallel computing.\n",
    "5. If you have a sequential (non-parallelizable computation) try some of the Just In Time (jit) compiler libraries like Numba. Numba can also compile to GPU CUDA code now too for a relatively easy pathway to GPU accelerated computation.\n",
    "\n",
    "\n",
    "## Other resources\n",
    "\n",
    "- I found Michael Kennedy's training courses really great. He has one for [aynchronous programming and parallelizing code](https://training.talkpython.fm/courses/explore_async_python/async-in-python-with-threading-and-multiprocessing) which I learnt a lot from (this is paid).\n",
    "- Plenty of other free materials - I like [Kaggle Learning](https://www.kaggle.com/learn/overview), [Real Python](https://realpython.com/), and for practising the real basic [Python Tutor](http://www.pythontutor.com/)\n",
    "- You can practise with most machine learning libraries and GPU accelerated code for free with [Google Colab](https://colab.research.google.com/) and [Kaggle Kernels](https://www.kaggle.com/kernels)\n",
    "- For nearly free content subscribe to [Humble Bundle](https://humblebundle.com/) they have great and extremely cheap boos and/or video coding and data science bundles almost monthly\n",
    "- There's plenty of great podcasts if you prefer learning in audio form - happy to give some recommendations if you ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
